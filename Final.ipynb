{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6K9t6LStITJN",
        "outputId": "115fe125-16b0-45ed-d6f4-b07df5ca555d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ƒê∆∞·ªùng d·∫´n ƒë·∫øn file zip tr√™n Google Drive c·ªßa b·∫°n\n",
        "zip_file_path = '/content/drive/MyDrive/data.zip'\n",
        "\n",
        "# Th∆∞ m·ª•c ƒë√≠ch ƒë·ªÉ gi·∫£i n√©n trong m√¥i tr∆∞·ªùng Colab\n",
        "# Ch√∫ng ta s·∫Ω gi·∫£i n√©n v√†o /content/dataset ƒë·ªÉ gi·ªØ m·ªçi th·ª© g·ªçn g√†ng\n",
        "extract_to_path = '/content/dataset'\n",
        "\n",
        "print(f\"ƒêang t·∫°o th∆∞ m·ª•c ƒë√≠ch: {extract_to_path}\")\n",
        "!mkdir -p {extract_to_path} # T·∫°o th∆∞ m·ª•c n·∫øu n√≥ ch∆∞a t·ªìn t·∫°i\n",
        "\n",
        "print(f\"ƒêang gi·∫£i n√©n {zip_file_path} v√†o {extract_to_path}...\")\n",
        "# L·ªánh unzip:\n",
        "# -q: quiet (kh√¥ng hi·ªÉn th·ªã qu√° nhi·ªÅu th√¥ng tin)\n",
        "# -d {extract_to_path}: ch·ªâ ƒë·ªãnh th∆∞ m·ª•c ƒë√≠ch ƒë·ªÉ gi·∫£i n√©n\n",
        "!unzip -q {zip_file_path} -d {extract_to_path}\n",
        "print(\"Gi·∫£i n√©n ho√†n t·∫•t.\")\n",
        "\n",
        "# Ki·ªÉm tra c·∫•u tr√∫c th∆∞ m·ª•c sau khi gi·∫£i n√©n (t√πy ch·ªçn)\n",
        "print(\"\\nKi·ªÉm tra c·∫•u tr√∫c th∆∞ m·ª•c sau gi·∫£i n√©n:\")\n",
        "!ls -R {extract_to_path}"
      ],
      "metadata": {
        "id": "eGkVZHNyJW88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =================== C√ÄI ƒê·∫∂T & ƒê·ªäNH NGHƒ®A =================== #\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from torch.optim import AdamW\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score\n",
        "from collections import Counter\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# =================== DATASET =================== #\n",
        "class MultimodalEmotionDataset(Dataset):\n",
        "    def __init__(self, data_dir, tokenizer, transform=None, max_length=100):\n",
        "        self.data_dir = data_dir\n",
        "        self.tokenizer = tokenizer\n",
        "        self.transform = transform\n",
        "        self.max_length = max_length\n",
        "        self.samples = []\n",
        "        self.label2id = {\"ti√™u c·ª±c\": 0, \"trung t√≠nh\": 1, \"t√≠ch c·ª±c\": 2}\n",
        "        labels_df = pd.read_csv(os.path.join(data_dir, \"label.csv\"))\n",
        "        for _, row in labels_df.iterrows():\n",
        "            id_ = str(row['ID'])\n",
        "            label = str(row['label']).strip().lower()\n",
        "            if label in self.label2id:\n",
        "                text_path = os.path.join(data_dir, \"texts\", f\"{id_}.txt\")\n",
        "                image_path = os.path.join(data_dir, \"images\", f\"{id_}.jpg\")\n",
        "                if os.path.exists(text_path) and os.path.exists(image_path):\n",
        "                    self.samples.append((id_, label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        id_, label = self.samples[idx]\n",
        "        with open(os.path.join(self.data_dir, \"texts\", f\"{id_}.txt\"), encoding='utf-8') as f:\n",
        "            text = f.read().strip()\n",
        "        encoded = self.tokenizer(\n",
        "            text, padding=\"max_length\", truncation=True,\n",
        "            max_length=self.max_length, return_tensors=\"pt\"\n",
        "        )\n",
        "        image = Image.open(os.path.join(self.data_dir, \"images\", f\"{id_}.jpg\")).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return {\n",
        "            \"input_ids\": encoded[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": encoded[\"attention_mask\"].squeeze(0),\n",
        "            \"image\": image,\n",
        "            \"label\": torch.tensor(self.label2id[label])\n",
        "        }\n",
        "\n",
        "# =================== ENCODERS =================== #\n",
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, model_name='vinai/phobert-base', output_dim=256, dropout_rate=0.2, pooling='mean', freeze_bert=False):\n",
        "        super(TextEncoder, self).__init__()\n",
        "        self.phobert = AutoModel.from_pretrained(model_name)\n",
        "        self.pooling = pooling\n",
        "        self.linear = nn.Linear(self.phobert.config.hidden_size, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.gelu = nn.GELU()\n",
        "        if freeze_bert:\n",
        "            for param in self.phobert.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.phobert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last_hidden_state = outputs.last_hidden_state\n",
        "        if self.pooling == 'cls':\n",
        "            pooled_output = last_hidden_state[:, 0]\n",
        "        elif self.pooling == 'mean':\n",
        "            mask = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
        "            pooled_output = (last_hidden_state * mask).sum(1) / mask.sum(1)\n",
        "        x = self.dropout(pooled_output)\n",
        "        x = self.linear(x)\n",
        "        x = self.gelu(x)\n",
        "        return x\n",
        "\n",
        "class ImageEncoder(nn.Module):\n",
        "    def __init__(self, backbone_name='resnet18', output_dim=256, dropout_rate=0.2, unfreeze_blocks=2, pretrained=True):\n",
        "        super(ImageEncoder, self).__init__()\n",
        "        if backbone_name == 'resnet18':\n",
        "            weights = models.ResNet18_Weights.IMAGENET1K_V1 if pretrained else None\n",
        "            self.backbone = models.resnet18(weights=weights)\n",
        "            in_features = 512\n",
        "        self.backbone = nn.Sequential(*list(self.backbone.children())[:-1])\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Linear(in_features, output_dim)\n",
        "        self.gelu = nn.GELU()\n",
        "        for param in self.backbone.parameters():\n",
        "            param.requires_grad = False\n",
        "        if unfreeze_blocks > 0:\n",
        "            for module in list(self.backbone.children())[-unfreeze_blocks:]:\n",
        "                for param in module.parameters():\n",
        "                    param.requires_grad = True\n",
        "        for param in self.fc.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        x = self.gelu(x)\n",
        "        return x\n",
        "\n",
        "# =================== FUSION MODEL =================== #\n",
        "class FusionClassifier(nn.Module):\n",
        "    def __init__(self, text_dim=256, image_dim=256, hidden_dim=256, num_classes=3, dropout_rate=0.2, num_heads=4):\n",
        "        super(FusionClassifier, self).__init__()\n",
        "        self.text_proj = nn.Linear(text_dim, hidden_dim)\n",
        "        self.image_proj = nn.Linear(image_dim, hidden_dim)\n",
        "        self.attention = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, dropout=dropout_rate, batch_first=True)\n",
        "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
        "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
        "        self.gate = nn.Sequential(nn.Linear(hidden_dim * 2, hidden_dim), nn.Sigmoid())\n",
        "        self.fusion_mlp = nn.Sequential(nn.Linear(hidden_dim * 2, hidden_dim), nn.GELU(), nn.Dropout(dropout_rate), nn.LayerNorm(hidden_dim))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2), nn.GELU(), nn.Dropout(dropout_rate),\n",
        "            nn.LayerNorm(hidden_dim // 2), nn.Linear(hidden_dim // 2, num_classes))\n",
        "\n",
        "    def forward(self, text_features, image_features):\n",
        "        text_embed = self.text_proj(text_features)\n",
        "        image_embed = self.image_proj(image_features)\n",
        "        attn_output, _ = self.attention(text_embed.unsqueeze(1), image_embed.unsqueeze(1), image_embed.unsqueeze(1))\n",
        "        attn_output = attn_output.squeeze(1)\n",
        "        attn_output = self.norm1(attn_output + text_embed)\n",
        "        concat = torch.cat([attn_output, image_embed], dim=1)\n",
        "        gate = self.gate(concat)\n",
        "        fused = gate * attn_output + (1 - gate) * image_embed\n",
        "        fused = self.fusion_mlp(concat)\n",
        "        fused = self.norm2(fused)\n",
        "        return self.classifier(fused)\n",
        "\n",
        "# =================== HU·∫§N LUY·ªÜN =================== #\n",
        "# THAM S·ªê\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "LR_TEXT_IMAGE = 2e-5\n",
        "LR_FUSION = 1e-4\n",
        "WEIGHT_DECAY = 1e-2\n",
        "GRAD_CLIP = 1.0\n",
        "TRAIN_DATA_DIR = \"/content/dataset/data/train\"\n",
        "VAL_DATA_DIR = \"/content/dataset/data/val\"\n",
        "CHECKPOINT_SAVE_DIR = \"/content/checkpoints\"\n",
        "\n",
        "# Tokenizer & transform\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
        "train_transform = transforms.Compose([transforms.RandomResizedCrop((224, 224)), transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                                                                   [0.229, 0.224, 0.225])])\n",
        "val_transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor(),\n",
        "                                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "\n",
        "train_dataset = MultimodalEmotionDataset(TRAIN_DATA_DIR, tokenizer, train_transform)\n",
        "val_dataset = MultimodalEmotionDataset(VAL_DATA_DIR, tokenizer, val_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "# Class weights\n",
        "label_counts = Counter([label for _, label in train_dataset.samples])\n",
        "class_counts = [label_counts[l] for l in [\"ti√™u c·ª±c\", \"trung t√≠nh\", \"t√≠ch c·ª±c\"]]\n",
        "class_weights = 1. / torch.tensor(class_counts, dtype=torch.float)\n",
        "class_weights = class_weights / class_weights.sum() * 3\n",
        "\n",
        "# Model\n",
        "text_encoder = TextEncoder().to(DEVICE)\n",
        "image_encoder = ImageEncoder().to(DEVICE)\n",
        "fusion_model = FusionClassifier().to(DEVICE)\n",
        "\n",
        "optimizer = AdamW([\n",
        "    {'params': text_encoder.parameters(), 'lr': LR_TEXT_IMAGE},\n",
        "    {'params': image_encoder.parameters(), 'lr': LR_TEXT_IMAGE},\n",
        "    {'params': fusion_model.parameters(), 'lr': LR_FUSION}\n",
        "], weight_decay=WEIGHT_DECAY)\n",
        "loss_fn = nn.CrossEntropyLoss(weight=class_weights.to(DEVICE))\n",
        "scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-6)\n",
        "\n",
        "# H√†m train v√† eval\n",
        "def train_one_epoch(model, dataloader, loss_fn, device, text_encoder, image_encoder, optimizer):\n",
        "    model.train()\n",
        "    text_encoder.train()\n",
        "    image_encoder.train()\n",
        "\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for batch in dataloader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        images = batch[\"image\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "\n",
        "        text_feat = text_encoder(input_ids, attention_mask)\n",
        "        image_feat = image_encoder(images)\n",
        "        logits = model(text_feat, image_feat)\n",
        "\n",
        "        loss = loss_fn(logits, labels)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(\n",
        "            list(text_encoder.parameters()) + list(image_encoder.parameters()) + list(model.parameters()),\n",
        "            GRAD_CLIP\n",
        "        )\n",
        "        optimizer.step()\n",
        "\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    acc = correct / total * 100\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "    return avg_loss, acc, f1\n",
        "\n",
        "\n",
        "def evaluate_model(model, dataloader, loss_fn, device, text_encoder, image_encoder):\n",
        "    model.eval()\n",
        "    text_encoder.eval()\n",
        "    image_encoder.eval()\n",
        "\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            images = batch[\"image\"].to(device)\n",
        "            labels = batch[\"label\"].to(device)\n",
        "\n",
        "            text_feat = text_encoder(input_ids, attention_mask)\n",
        "            image_feat = image_encoder(images)\n",
        "            logits = model(text_feat, image_feat)\n",
        "\n",
        "            loss = loss_fn(logits, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    acc = correct / total * 100\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "    return avg_loss, acc, f1\n",
        "\n",
        "# Training loop\n",
        "best_val_f1 = 0.0\n",
        "patience = 8\n",
        "trigger_times = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss, train_acc, train_f1 = train_one_epoch(fusion_model, train_loader, loss_fn, DEVICE,\n",
        "                                                      text_encoder, image_encoder, optimizer)\n",
        "    val_loss, val_acc, val_f1 = evaluate_model(fusion_model, val_loader, loss_fn, DEVICE,\n",
        "                                               text_encoder, image_encoder)\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} - Train F1: {train_f1:.4f} - Val F1: {val_f1:.4f}\")\n",
        "    if val_f1 > best_val_f1:\n",
        "        best_val_f1 = val_f1\n",
        "        os.makedirs(CHECKPOINT_SAVE_DIR, exist_ok=True)\n",
        "        torch.save({\n",
        "            'text_encoder': text_encoder,\n",
        "            'image_encoder': image_encoder,\n",
        "            'fusion_model': fusion_model\n",
        "        }, os.path.join(CHECKPOINT_SAVE_DIR, \"best_multimodal_model.pt\"))\n",
        "        print(f\"‚úÖ L∆∞u model t·∫°i epoch {epoch+1} v·ªõi F1 = {val_f1:.4f}\")\n",
        "        trigger_times = 0\n",
        "    else:\n",
        "        trigger_times += 1\n",
        "        if trigger_times >= patience:\n",
        "            print(\"‚õîÔ∏è Early stopping.\")\n",
        "            break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e70eDyMBLVBq",
        "outputId": "cd8cf6d0-ad64-41a5-90c9-42616e0f38c1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Train F1: 0.4591 - Val F1: 0.5994\n",
            "‚úÖ L∆∞u model t·∫°i epoch 1 v·ªõi F1 = 0.5994\n",
            "Epoch 2 - Train F1: 0.6678 - Val F1: 0.6406\n",
            "‚úÖ L∆∞u model t·∫°i epoch 2 v·ªõi F1 = 0.6406\n",
            "Epoch 3 - Train F1: 0.7928 - Val F1: 0.7090\n",
            "‚úÖ L∆∞u model t·∫°i epoch 3 v·ªõi F1 = 0.7090\n",
            "Epoch 4 - Train F1: 0.8199 - Val F1: 0.7080\n",
            "Epoch 5 - Train F1: 0.8797 - Val F1: 0.7269\n",
            "‚úÖ L∆∞u model t·∫°i epoch 5 v·ªõi F1 = 0.7269\n",
            "Epoch 6 - Train F1: 0.9137 - Val F1: 0.7086\n",
            "Epoch 7 - Train F1: 0.9354 - Val F1: 0.7001\n",
            "Epoch 8 - Train F1: 0.9482 - Val F1: 0.7300\n",
            "‚úÖ L∆∞u model t·∫°i epoch 8 v·ªõi F1 = 0.7300\n",
            "Epoch 9 - Train F1: 0.9548 - Val F1: 0.7112\n",
            "Epoch 10 - Train F1: 0.9677 - Val F1: 0.7149\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "checkpoint = torch.load(\"/content/checkpoints/best_multimodal_model.pt\", map_location=\"cpu\", weights_only=False)\n",
        "\n",
        "text_encoder = checkpoint[\"text_encoder\"].eval()\n",
        "image_encoder = checkpoint[\"image_encoder\"].eval()\n",
        "fusion_model = checkpoint[\"fusion_model\"].eval()\n"
      ],
      "metadata": {
        "id": "eF51YqDMMwta"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from transformers import AutoTokenizer\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "\n",
        "ID = \"1022\"\n",
        "test_dir = \"/content/dataset/data/test\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
        "\n",
        "# Chu·∫©n b·ªã ·∫£nh\n",
        "image_path = f\"{test_dir}/images/{ID}.jpg\"\n",
        "image = Image.open(image_path).convert(\"RGB\")\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "image_tensor = transform(image).unsqueeze(0)  # shape [1, 3, 224, 224]\n",
        "\n",
        "# Chu·∫©n b·ªã text\n",
        "with open(f\"{test_dir}/texts/{ID}.txt\", encoding='utf-8') as f:\n",
        "    text = f.read().strip()\n",
        "\n",
        "encoded = tokenizer(text, padding=\"max_length\", truncation=True,\n",
        "                    max_length=100, return_tensors=\"pt\")\n",
        "input_ids = encoded[\"input_ids\"]  # shape [1, 100]\n",
        "attention_mask = encoded[\"attention_mask\"]\n"
      ],
      "metadata": {
        "id": "oJhMGyn3M0Q5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    text_feat = text_encoder(input_ids, attention_mask)\n",
        "    image_feat = image_encoder(image_tensor)\n",
        "    logits = fusion_model(text_feat, image_feat)\n",
        "    predicted = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "# Mapping ID to label\n",
        "id2label = {0: \"ti√™u c·ª±c\", 1: \"trung t√≠nh\", 2: \"t√≠ch c·ª±c\"}\n",
        "print(f\"üñºÔ∏è ID = {ID}\")\n",
        "print(f\"üìÑ VƒÉn b·∫£n: {text}\")\n",
        "print(f\"üîÆ D·ª± ƒëo√°n c·∫£m x√∫c: {id2label[predicted]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNFGmjfLNIih",
        "outputId": "da01a677-ea55-46cc-c447-0334a2de40f1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üñºÔ∏è ID = 1022\n",
            "üìÑ VƒÉn b·∫£n: t√¥i kh√¥ng n√≥i n√™n l·ªùi ... chapelhillshooting muslimlivesmatter usmedia\n",
            "üîÆ D·ª± ƒëo√°n c·∫£m x√∫c: t√≠ch c·ª±c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "checkpoint = torch.load(\"/content/checkpoints/best_multimodal_model.pt\", map_location=\"cuda\" if torch.cuda.is_available() else \"cpu\", weights_only=False)\n",
        "text_encoder = checkpoint['text_encoder'].to(\"cuda\" if torch.cuda.is_available() else \"cpu\").eval()\n",
        "image_encoder = checkpoint['image_encoder'].to(\"cuda\" if torch.cuda.is_available() else \"cpu\").eval()\n",
        "fusion_model = checkpoint['fusion_model'].to(\"cuda\" if torch.cuda.is_available() else \"cpu\").eval()\n"
      ],
      "metadata": {
        "id": "xpdB_BpmOwe_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Thi·∫øt l·∫≠p thi·∫øt b·ªã\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ƒê∆∞a m√¥ h√¨nh v·ªÅ ƒë√∫ng thi·∫øt b·ªã v√† chuy·ªÉn sang ch·∫ø ƒë·ªô eval\n",
        "text_encoder = checkpoint['text_encoder'].to(device).eval()\n",
        "image_encoder = checkpoint['image_encoder'].to(device).eval()\n",
        "fusion_model = checkpoint['fusion_model'].to(device).eval()\n",
        "\n",
        "# Kh·ªüi t·∫°o danh s√°ch l∆∞u k·∫øt qu·∫£\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "# V√≤ng l·∫∑p ƒë√°nh gi√°\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        images = batch['image'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        # Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng v√† d·ª± ƒëo√°n\n",
        "        text_feat = text_encoder(input_ids, attention_mask)\n",
        "        image_feat = image_encoder(images)\n",
        "        outputs = fusion_model(text_feat, image_feat)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "        # L∆∞u k·∫øt qu·∫£\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "# In k·∫øt qu·∫£ ƒë√°nh gi√°\n",
        "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=[\"ti√™u c·ª±c\", \"trung t√≠nh\", \"t√≠ch c·ª±c\"]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6fMxE44OWQH",
        "outputId": "13efe9f1-dd98-4d92-bb54-ec881cacc046"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7475247524752475\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    ti√™u c·ª±c       0.65      0.61      0.63        54\n",
            "  trung t√≠nh       0.63      0.67      0.65        36\n",
            "    t√≠ch c·ª±c       0.83      0.84      0.84       112\n",
            "\n",
            "    accuracy                           0.75       202\n",
            "   macro avg       0.70      0.71      0.70       202\n",
            "weighted avg       0.75      0.75      0.75       202\n",
            "\n"
          ]
        }
      ]
    }
  ]
}